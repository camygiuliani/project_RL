


====16 Gennaio========
--vedere se plot sarfa è cambiato... se è più simile al paper? [OK]
-- mandare mail [in proggress]

=== 17 gennaio ===

- quante immagini sarfa?
- fare qualche appunto sugli algoritmi

======== 18 Gen========
risposta tutor:
Potrebbe andare bene confrontare PPO e un algoritmo off-policy.
Magari potreste partire con PPO e SAC, ed eventualmente estendere 
l’analisi a DDQN se c’è tempo, visto che è comunque off-policy, ma è value-based.

=========== 25 gen ===========
-sistma print durante training  [OK]
- controllare paper
-grafici

=========== 27 gen ===========

riorganizzare gli argomenti delle funzioni init e train in ppo e sac, 
aggiungendo la directory per i checkpoint [OK]

- next_eval meccanismo pure sugli altri oltre sac
-render per eval[OK]

===== 31 Gen    ==============
- problema csv train di DDQN
- azioni sarfa?


> Inserire nel readme per alsa comandi apt ===========
sudo apt install -y alsa-utils libasound2t64
sudo apt install -y pulseaudio


==== 3 Feb ====
Add vectorized environment on PPO and DDQN for better training 

============= 9 Feb
- allenare sac 6 ore
- capire se si può fixare video
- analisi stabilità?


Michela:
Questo tipo di analisi sarebbe molto interessante.
Vi suggerirei di fare la seguente cosa:
- Oscurare patch casuali per verificare la robustezza/stabilità degli algoritmi testati, che rappresenterà la vostra baseline.
- Oscurare le parti identificate come importanti da SARFA, per vedere se sono effettivamente i dettagli su cui gli algoritmi si stanno basando per prendere le decisioni. Se effettivamente la diminuzione in performance è maggiore, potete poi fare analisi del tipo: "se un algoritmo si concentra su poche regioni importanti (sparsity) mentre un altro no, rimuovendo lo stesso numero di patch porterà ad un degrado maggiore nel primo piuttosto che nel secondo" ecc.
In questo modo avrete sia il confronto diretto tra i metodi, che un'analisi più specifica su SARFA, che è l'approccio "non visto a lezione" su cui vi siete concentrate. Se avete tempo, potete anche fare dei grafici con la metrica di degrado che sceglierete sull'asse y (es. delta return, delta return percentuale - di quanto aumenta/diminuisce il return rispetto a quello non perturbed, delta success rate, o altro che sceglierete in base all'environment che state considerando) e la percentuale di osservazione mascherata sull'asse x. Così vedrete quanto sono robuste le decisioni all'aumentare del rumore.
Una cosa a cui dovrete fare attenzione è come mascherate. Ad esempio, mascherare con tutti 0 (patch nera) nel caso di chess genera esempi OOD, che potrebbero portare ad un'analisi poco indicativa. Una cosa che potreste fare è mascherare sostituendo celle "piene" (con una pedina) con celle vuote con lo stesso colore di background, oppure sostituendo la patch con una patch con la media locale dei pixel in quella patch.

==========================

problema sac:

[step 400000] Avg100=4.950, q1=2714130159513894912.000 q2=560537255474064523264.000 Actor=-44761243648.000 Alpha=911724096.000 Entropy=1.757
[step 410000] Avg100=5.240, q1=2154363842010808320.000 q2=540707308180426719232.000 Actor=-43705794560.000 Alpha=911432000.000 Entropy=1.757
[step 420000] Avg100=5.250, q1=5286234752800522240.000 q2=561743235011781328896.000 Actor=-44827533312.000 Alpha=907475200.000 Entropy=1.755
[step 430000] Avg100=4.910, q1=3653017157245075456.000 q2=541933448363350425600.000 Actor=-44787392512.000 Alpha=903144640.000 Entropy=1.756
[step 440000] Avg100=5.110, q1=6651026650521665536.000 q2=550480998981122916352.000 Actor=-43820978176.000 Alpha=905811648.000 Entropy=1.756
[step 450000] Avg100=5.550, q1=2236766466076573696.000 q2=563123447560082030592.000 Actor=-45827854336.000 Alpha=907657024.000 Entropy=1.753
[step 460000] Avg100=5.340, q1=5471384264580792320.000 q2=534613374934641016832.000 Actor=-43124400128.000 Alpha=903421952.000 Entropy=1.756
[step 470000] Avg100=5.630, q1=2697921433975128064.000 q2=556419874699113136128.000 Actor=-45591785472.000 Alpha=907118720.000 Entropy=1.757
[step 480000] Avg100=5.720, q1=2549255642149814272.000 q2=560848109401469353984.000 Actor=-45411262464.000 Alpha=903080896.000 Entropy=1.755
[step 490000] Avg100=5.520, q1=6175764398682931200.000 q2=559993481003431624704.000 Actor=-45304102912.000 Alpha=903103296.000 Entropy=1.755
[step 500000] Avg100=5.260, q1=4320917144805048320.000 q2=526386776159062851584.000 Actor=-44296101888.000 Alpha=903968384.000 Entropy=1.755
[step 510000] Avg100=5.210, q1=3952775088180297728.000 q2=569543962147596271616.000 Actor=-45842792448.000 Alpha=904054592.000 Entropy=1.756 
[step 520000] Avg100=5.470, q1=9555699426350071808.000 q2=580876285447455637504.000 Actor=-46026039296.000 Alpha=904166656.000 Entropy=1.757
[step 530000] Avg100=5.250, q1=4490586632377860096.000 q2=558038179893338963968.000 Actor=-45062234112.000 Alpha=901979136.000 Entropy=1.755
[step 540000] Avg100=5.580, q1=3970764747678154752.000 q2=549794094484832649216.000 Actor=-45153882112.000 Alpha=904413312.000 Entropy=1.757 
[step 550000] Avg100=5.710, q1=4979971286441656320.000 q2=557392617034253074432.000 Actor=-44490272768.000 Alpha=900897600.000 Entropy=1.757 
pha=902596992.000 Entropy=1.756